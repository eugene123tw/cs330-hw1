{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMixHTKuraXAQcSkhfmPjW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eugene123tw/cs330-hw1/blob/master/HW1_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkn5pV0NTBmK",
        "colab_type": "code",
        "outputId": "123c97ae-be37-45d6-9fad-b6664c611f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Downlaod Omniglot \n",
        "workspace_dir = '.'\n",
        "!gdown --id 1aBacYkuigdlKExME-kgxqworbdd8Zixg --output \"{workspace_dir}/omniglot_resized.zip\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aBacYkuigdlKExME-kgxqworbdd8Zixg\n",
            "To: /content/omniglot_resized.zip\n",
            "\r0.00B [00:00, ?B/s]\r13.0MB [00:00, 205MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K7pU3JcXcqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q omniglot_resized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZFUsipoGA4B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "2665e722-70eb-4303-a6c7-fb88bfe70e44"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr  9 13:32:03 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc5fNODB-HfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "a2b32096-8308-4250-948c-700afad86bc9"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "\n",
        "def get_images(paths, labels, nb_samples=None, shuffle=True):\n",
        "  \"\"\"\n",
        "  Takes a set of character folders and labels and returns paths to image files\n",
        "  paired with labels.\n",
        "  Args:\n",
        "      paths: A list of character folders\n",
        "      labels: List or numpy array of same length as paths\n",
        "      nb_samples: Number of images to retrieve per character\n",
        "  Returns:\n",
        "      List of (label, image_path) tuples\n",
        "  \"\"\"\n",
        "  if nb_samples is not None:\n",
        "    sampler = lambda x: random.sample(x, nb_samples)\n",
        "  else:\n",
        "    sampler = lambda x: x\n",
        "  images_labels = [(i, os.path.join(path, image))\n",
        "                   for i, path in zip(labels, paths)\n",
        "                   for image in sampler(os.listdir(path))]\n",
        "  if shuffle:\n",
        "    random.shuffle(images_labels)\n",
        "  return images_labels\n",
        "\n",
        "\n",
        "def image_file_to_array(filename, dim_input):\n",
        "  \"\"\"\n",
        "  Takes an image path and returns numpy array\n",
        "  Args:\n",
        "      filename: Image filename\n",
        "      dim_input: Flattened shape of image\n",
        "  Returns:\n",
        "      1 channel image\n",
        "  \"\"\"\n",
        "  image = imread(filename)\n",
        "  image = image.reshape([dim_input])\n",
        "  image = image.astype(np.float32) / 255.0\n",
        "  image = 1.0 - image\n",
        "  return image\n",
        "\n",
        "\n",
        "class DataGenerator(object):\n",
        "  \"\"\"\n",
        "  Data Generator capable of generating batches of Omniglot data.\n",
        "  A \"class\" is considered a class of omniglot digits.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_classes, num_samples_per_class, config={}):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        num_classes: Number of classes for classification (K-way)\n",
        "        num_samples_per_class: num samples to generate per class in one batch\n",
        "        batch_size: size of meta batch size (e.g. number of functions)\n",
        "    \"\"\"\n",
        "    self.num_samples_per_class = num_samples_per_class\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    data_folder = config.get('data_folder', './omniglot_resized')\n",
        "    self.img_size = config.get('img_size', (28, 28))\n",
        "\n",
        "    self.dim_input = np.prod(self.img_size)\n",
        "    self.dim_output = self.num_classes\n",
        "\n",
        "    character_folders = [os.path.join(data_folder, family, character)\n",
        "                         for family in os.listdir(data_folder)\n",
        "                         if os.path.isdir(os.path.join(data_folder, family))\n",
        "                         for character in os.listdir(os.path.join(data_folder, family))\n",
        "                         if os.path.isdir(os.path.join(data_folder, family, character))]\n",
        "\n",
        "    random.seed(1)\n",
        "    random.shuffle(character_folders)\n",
        "    num_val = 100\n",
        "    num_train = 1100\n",
        "    self.metatrain_character_folders = character_folders[: num_train]\n",
        "    self.metaval_character_folders = character_folders[\n",
        "                                     num_train:num_train + num_val]\n",
        "    self.metatest_character_folders = character_folders[\n",
        "                                      num_train + num_val:]\n",
        "\n",
        "  def sample_batch(self, batch_type, batch_size):\n",
        "    \"\"\"\n",
        "    Samples a batch for training, validation, or testing\n",
        "    Args:\n",
        "        batch_type: train/val/test\n",
        "    Returns:\n",
        "        A a tuple of (1) Image batch and (2) Label batch where\n",
        "        image batch has shape [B, K, N, 784] and label batch has shape [B, K, N, N]\n",
        "        where B is batch size, K is number of samples per class, N is number of classes\n",
        "    \"\"\"\n",
        "    if batch_type == \"train\":\n",
        "      folders = self.metatrain_character_folders\n",
        "    elif batch_type == \"val\":\n",
        "      folders = self.metaval_character_folders\n",
        "    else:\n",
        "      folders = self.metatest_character_folders\n",
        "\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "    # Initialise array for storage\n",
        "    all_image_batches = np.zeros(\n",
        "      (batch_size, self.num_samples_per_class, self.num_classes, 784)\n",
        "    )\n",
        "\n",
        "    all_label_batches = np.zeros(\n",
        "      (batch_size, self.num_samples_per_class, self.num_classes, self.num_classes)\n",
        "    )\n",
        "\n",
        "    for b in range(batch_size):  # sample mini batch of tasks\n",
        "      sampled_paths = np.random.choice(folders, self.num_classes)\n",
        "      images_labels = get_images(\n",
        "        sampled_paths, np.eye(self.num_classes), self.num_samples_per_class, False)\n",
        "\n",
        "      for k in range(self.num_samples_per_class):\n",
        "        s = slice(k, len(images_labels), self.num_samples_per_class)\n",
        "        kth_images_labels = images_labels[s]\n",
        "        np.random.shuffle(kth_images_labels)\n",
        "        for n, (y_vector, fname) in enumerate(kth_images_labels):\n",
        "          all_image_batches[b, k, n, :] = image_file_to_array(fname, 784)\n",
        "          all_label_batches[b, k, n, :] = y_vector\n",
        "    #############################\n",
        "\n",
        "    return all_image_batches, all_label_batches"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAdYWhhJ_MUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(preds, real_labels):\n",
        "  \"\"\"\n",
        "  Computes MANN loss\n",
        "  Args:\n",
        "      preds: [B, K+1, N, N] network output\n",
        "      labels: [B, K+1, N, N] labels\n",
        "  Returns:\n",
        "      scalar loss\n",
        "  \"\"\"\n",
        "  #############################\n",
        "  #### YOUR CODE GOES HERE ####\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "  b, k, n, n = real_labels.get_shape().as_list()\n",
        "  labels = tf.reshape(real_labels, (-1, k * n, n))\n",
        "  return loss(labels[:, -1:], preds[:, -1:])\n",
        "  #############################\n",
        "\n",
        "\n",
        "class MANN(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_classes, samples_per_class):\n",
        "    super(MANN, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.samples_per_class = samples_per_class\n",
        "    self.layer1 = tf.keras.layers.LSTM(128, return_sequences=True)\n",
        "    self.layer2 = tf.keras.layers.LSTM(num_classes, return_sequences=True)\n",
        "\n",
        "  def call(self, input_images, fake_labels):\n",
        "    \"\"\"\n",
        "    MANN\n",
        "    Args:\n",
        "        input_images: [B, K+1, N, 11025] flattened images\n",
        "        labels: [B, K+1, N, N] ground truth labels\n",
        "    Returns:\n",
        "        [B, K+1, N, N] predictions\n",
        "    \"\"\"\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "    K = self.samples_per_class\n",
        "    N = self.num_classes\n",
        "    input_images = tf.reshape(input_images, (-1, K * N, 784))\n",
        "    fake_labels = tf.reshape(fake_labels, (-1, K * N, N))\n",
        "    input_tensor = tf.concat([input_images, fake_labels], axis=2)\n",
        "    out = self.layer1(input_tensor)\n",
        "    out = self.layer2(out)\n",
        "    #############################\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG_PDNsK_XZZ",
        "colab_type": "text"
      },
      "source": [
        "# Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpAHnCzI_TXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Process some flags.')\n",
        "\n",
        "parser.add_argument('--num_classes', type=int, default=5, help='number of classes used in classification (e.g. 5-way classification).')\n",
        "\n",
        "parser.add_argument('--num_samples', type=int, default=1, help='number of examples used for inner gradient update (K for K-shot learning).')\n",
        "\n",
        "parser.add_argument('--meta_batch_size', type=int, default=16, help='Number of N-way classification tasks per batch')\n",
        "\n",
        "parser.add_argument('--data_root', type=str, default= 'omniglot_resized', help='data folder root')\n",
        "\n",
        "FLAGS = parser.parse_args(args=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yceZeghb_OHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5fbfd4e7-494e-4661-9a03-4e28e3898826"
      },
      "source": [
        "ims = tf.placeholder(tf.float32, shape=(None, FLAGS.num_samples + 1, FLAGS.num_classes, 784))\n",
        "\n",
        "fake_labels = tf.placeholder(tf.float32, shape=(None, FLAGS.num_samples + 1, FLAGS.num_classes, FLAGS.num_classes))\n",
        "\n",
        "real_labels = tf.placeholder(tf.float32, shape=(None, FLAGS.num_samples + 1, FLAGS.num_classes, FLAGS.num_classes))\n",
        "\n",
        "data_generator = DataGenerator(\n",
        "  FLAGS.num_classes,\n",
        "  FLAGS.num_samples + 1,\n",
        "  {'data_folder': FLAGS.data_root}\n",
        ")\n",
        "\n",
        "o = MANN(FLAGS.num_classes, FLAGS.num_samples + 1)\n",
        "out = o(ims, fake_labels)\n",
        "\n",
        "loss = loss_function(out, real_labels)\n",
        "optim = tf.train.AdamOptimizer(0.001)\n",
        "optimizer_step = optim.minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.local_variables_initializer())\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for step in range(50000):\n",
        "    i, l = data_generator.sample_batch('train', FLAGS.meta_batch_size)\n",
        "    l_fake = l.copy()\n",
        "    l_fake[:, -1:] = 0\n",
        "    feed = {\n",
        "      ims: i.astype(np.float32),\n",
        "      fake_labels: l_fake.astype(np.float32),\n",
        "      real_labels: l.astype(np.float32),\n",
        "    }\n",
        "    _, ls = sess.run([optimizer_step, loss], feed)\n",
        "\n",
        "    if step % 100 == 0:\n",
        "      print(\"*\" * 5 + \"Iter \" + str(step) + \"*\" * 5)\n",
        "      i, l = data_generator.sample_batch('test', 100)\n",
        "      l_fake = l.copy()\n",
        "      l_fake[:, -1:] = 0\n",
        "      feed = {\n",
        "        ims: i.astype(np.float32),\n",
        "        fake_labels: l_fake.astype(np.float32),\n",
        "        real_labels: l.astype(np.float32),\n",
        "      }\n",
        "      pred, tls = sess.run([out, loss], feed)\n",
        "      print(\"Train Loss:\", ls, \"Test Loss:\", tls)\n",
        "      pred = pred.reshape(\n",
        "        -1, FLAGS.num_samples + 1,\n",
        "        FLAGS.num_classes, FLAGS.num_classes)\n",
        "      pred = pred[:, -1, :, :].argmax(2)\n",
        "      l = l[:, -1, :, :].argmax(2)\n",
        "      print(\"Test Accuracy\", (1.0 * (pred == l)).mean())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****Iter 0*****\n",
            "Train Loss: 1.587503 Test Loss: 1.6143907\n",
            "Test Accuracy 0.216\n",
            "*****Iter 100*****\n",
            "Train Loss: 1.6186326 Test Loss: 1.6091939\n",
            "Test Accuracy 0.198\n",
            "*****Iter 200*****\n",
            "Train Loss: 1.6094382 Test Loss: 1.6094393\n",
            "Test Accuracy 0.202\n",
            "*****Iter 300*****\n",
            "Train Loss: 1.609438 Test Loss: 1.6094381\n",
            "Test Accuracy 0.2\n",
            "*****Iter 400*****\n",
            "Train Loss: 1.6094383 Test Loss: 1.6094377\n",
            "Test Accuracy 0.204\n",
            "*****Iter 500*****\n",
            "Train Loss: 1.6094382 Test Loss: 1.6094382\n",
            "Test Accuracy 0.202\n",
            "*****Iter 600*****\n",
            "Train Loss: 1.6094382 Test Loss: 1.6094383\n",
            "Test Accuracy 0.198\n",
            "*****Iter 700*****\n",
            "Train Loss: 1.609438 Test Loss: 1.6094377\n",
            "Test Accuracy 0.2\n",
            "*****Iter 800*****\n",
            "Train Loss: 1.6094381 Test Loss: 1.6094377\n",
            "Test Accuracy 0.198\n",
            "*****Iter 900*****\n",
            "Train Loss: 1.6094382 Test Loss: 1.6094378\n",
            "Test Accuracy 0.198\n",
            "*****Iter 1000*****\n",
            "Train Loss: 1.609438 Test Loss: 1.6094382\n",
            "Test Accuracy 0.196\n",
            "*****Iter 1100*****\n",
            "Train Loss: 1.6094378 Test Loss: 1.6094378\n",
            "Test Accuracy 0.2\n",
            "*****Iter 1200*****\n",
            "Train Loss: 1.6094351 Test Loss: 1.6094378\n",
            "Test Accuracy 0.198\n",
            "*****Iter 1300*****\n",
            "Train Loss: 1.6094381 Test Loss: 1.6094381\n",
            "Test Accuracy 0.2\n",
            "*****Iter 1400*****\n",
            "Train Loss: 1.609438 Test Loss: 1.6094376\n",
            "Test Accuracy 0.2\n",
            "*****Iter 1500*****\n",
            "Train Loss: 1.6094377 Test Loss: 1.6094378\n",
            "Test Accuracy 0.198\n",
            "*****Iter 1600*****\n",
            "Train Loss: 1.609438 Test Loss: 1.6094377\n",
            "Test Accuracy 0.2\n",
            "*****Iter 1700*****\n",
            "Train Loss: 1.6094382 Test Loss: 1.6094378\n",
            "Test Accuracy 0.2\n",
            "*****Iter 1800*****\n",
            "Train Loss: 1.609438 Test Loss: 1.6094382\n",
            "Test Accuracy 0.198\n",
            "*****Iter 1900*****\n",
            "Train Loss: 1.609437 Test Loss: 1.6094376\n",
            "Test Accuracy 0.198\n",
            "*****Iter 2000*****\n",
            "Train Loss: 1.6094358 Test Loss: 1.6094375\n",
            "Test Accuracy 0.198\n",
            "*****Iter 2100*****\n",
            "Train Loss: 1.6094382 Test Loss: 1.6094382\n",
            "Test Accuracy 0.2\n",
            "*****Iter 2200*****\n",
            "Train Loss: 1.609438 Test Loss: 1.6094377\n",
            "Test Accuracy 0.2\n",
            "*****Iter 2300*****\n",
            "Train Loss: 1.609438 Test Loss: 1.6094372\n",
            "Test Accuracy 0.202\n",
            "*****Iter 2400*****\n",
            "Train Loss: 1.6094375 Test Loss: 1.6094377\n",
            "Test Accuracy 0.202\n",
            "*****Iter 2500*****\n",
            "Train Loss: 1.6094387 Test Loss: 1.6094376\n",
            "Test Accuracy 0.202\n",
            "*****Iter 2600*****\n",
            "Train Loss: 1.6094378 Test Loss: 1.6094381\n",
            "Test Accuracy 0.204\n",
            "*****Iter 2700*****\n",
            "Train Loss: 1.609438 Test Loss: 1.6094381\n",
            "Test Accuracy 0.196\n",
            "*****Iter 2800*****\n",
            "Train Loss: 1.6094378 Test Loss: 1.6094378\n",
            "Test Accuracy 0.198\n",
            "*****Iter 2900*****\n",
            "Train Loss: 1.609438 Test Loss: 1.6094381\n",
            "Test Accuracy 0.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vsfks1OCflJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(FLAGS.data_root)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}