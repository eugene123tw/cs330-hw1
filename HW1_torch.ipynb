{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_torch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOB5yDUv0rMWR5OVlUa1K8o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eugene123tw/cs330-hw1/blob/master/HW1_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQF_A-QXK98n",
        "colab_type": "code",
        "outputId": "1f9d9b62-52f3-499b-b9d5-33d2f50376d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Downlaod Omniglot \n",
        "workspace_dir = '.'\n",
        "!gdown --id 1aBacYkuigdlKExME-kgxqworbdd8Zixg --output \"{workspace_dir}/omniglot_resized.zip\"\n",
        "!unzip -q omniglot_resized"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aBacYkuigdlKExME-kgxqworbdd8Zixg\n",
            "To: /content/omniglot_resized.zip\n",
            "13.0MB [00:00, 49.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIaBbC75o6HT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7b4fc39-4fc9-4eaf-e317-e4a6f911127a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmdQh0OMLDrY",
        "colab_type": "code",
        "outputId": "8b9bab90-38f7-40a6-8ddb-2f1d42a0733a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Apr 11 14:30:54 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URsMk0fXYrMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "cuda = torch.device('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oUIO3LbLGAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_images(paths, labels, nb_samples=None, shuffle=True):\n",
        "  \"\"\"\n",
        "  Takes a set of character folders and labels and returns paths to image files\n",
        "  paired with labels.\n",
        "  Args:\n",
        "      paths: A list of character folders\n",
        "      labels: List or numpy array of same length as paths\n",
        "      nb_samples: Number of images to retrieve per character\n",
        "  Returns:\n",
        "      List of (label, image_path) tuples\n",
        "  \"\"\"\n",
        "  if nb_samples is not None:\n",
        "    sampler = lambda x: random.sample(x, nb_samples)\n",
        "  else:\n",
        "    sampler = lambda x: x\n",
        "  images_labels = [(i, os.path.join(path, image))\n",
        "                   for i, path in zip(labels, paths)\n",
        "                   for image in sampler(os.listdir(path))]\n",
        "  if shuffle:\n",
        "    random.shuffle(images_labels)\n",
        "  return images_labels\n",
        "\n",
        "\n",
        "def image_file_to_array(filename, dim_input):\n",
        "  \"\"\"\n",
        "  Takes an image path and returns numpy array\n",
        "  Args:\n",
        "      filename: Image filename\n",
        "      dim_input: Flattened shape of image\n",
        "  Returns:\n",
        "      1 channel image\n",
        "  \"\"\"\n",
        "  image = imread(filename)\n",
        "  image = image.reshape([dim_input])\n",
        "  image = image.astype(np.float32) / 255.0\n",
        "  image = 1.0 - image\n",
        "  return image\n",
        "\n",
        "\n",
        "class DataGenerator(object):\n",
        "  \"\"\"\n",
        "  Data Generator capable of generating batches of Omniglot data.\n",
        "  A \"class\" is considered a class of omniglot digits.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_classes, num_samples_per_class, config={}):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        num_classes: Number of classes for classification (K-way)\n",
        "        num_samples_per_class: num samples to generate per class in one batch\n",
        "        batch_size: size of meta batch size (e.g. number of functions)\n",
        "    \"\"\"\n",
        "    self.num_samples_per_class = num_samples_per_class\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    data_folder = config.get('data_folder', './omniglot_resized')\n",
        "    self.img_size = config.get('img_size', (28, 28))\n",
        "\n",
        "    self.dim_input = np.prod(self.img_size)\n",
        "    self.dim_output = self.num_classes\n",
        "\n",
        "    character_folders = [os.path.join(data_folder, family, character)\n",
        "                         for family in os.listdir(data_folder)\n",
        "                         if os.path.isdir(os.path.join(data_folder, family))\n",
        "                         for character in os.listdir(os.path.join(data_folder, family))\n",
        "                         if os.path.isdir(os.path.join(data_folder, family, character))]\n",
        "\n",
        "    random.seed(1)\n",
        "    random.shuffle(character_folders)\n",
        "    num_val = 100\n",
        "    num_train = 1100\n",
        "    self.metatrain_character_folders = character_folders[: num_train]\n",
        "    self.metaval_character_folders = character_folders[\n",
        "                                     num_train:num_train + num_val]\n",
        "    self.metatest_character_folders = character_folders[\n",
        "                                      num_train + num_val:]\n",
        "\n",
        "  def sample_batch(self, batch_type, batch_size):\n",
        "    \"\"\"\n",
        "    Samples a batch for training, validation, or testing\n",
        "    Args:\n",
        "        batch_type: train/val/test\n",
        "    Returns:\n",
        "        A a tuple of (1) Image batch and (2) Label batch where\n",
        "        image batch has shape [B, K, N, 784] and label batch has shape [B, K, N, N]\n",
        "        where B is batch size, K is number of samples per class, N is number of classes\n",
        "    \"\"\"\n",
        "    if batch_type == \"train\":\n",
        "      folders = self.metatrain_character_folders\n",
        "    elif batch_type == \"val\":\n",
        "      folders = self.metaval_character_folders\n",
        "    else:\n",
        "      folders = self.metatest_character_folders\n",
        "\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "    # Initialise array for storage\n",
        "    all_image_batches = np.zeros(\n",
        "      (batch_size, self.num_samples_per_class, self.num_classes, 784)\n",
        "    )\n",
        "\n",
        "    all_label_batches = np.zeros(\n",
        "      (batch_size, self.num_samples_per_class, self.num_classes, self.num_classes)\n",
        "    )\n",
        "\n",
        "    for b in range(batch_size):  # sample mini batch of tasks\n",
        "      sampled_paths = np.random.choice(folders, self.num_classes)\n",
        "      images_labels = get_images(\n",
        "        sampled_paths, np.eye(self.num_classes), self.num_samples_per_class, False)\n",
        "\n",
        "      test_images, test_labels = [], []\n",
        "      train_images, train_labels = [], []\n",
        "      for i, (y_vector, image_path) in enumerate(images_labels):\n",
        "        image = image_file_to_array(image_path, 784)\n",
        "        if i % self.num_samples_per_class == 0:\n",
        "          test_images.append(image)\n",
        "          test_labels.append(y_vector)\n",
        "        else:\n",
        "          train_images.append(image)\n",
        "          train_labels.append(y_vector)\n",
        "\n",
        "      ts_indices = np.random.permutation(range(len(test_images)))\n",
        "      test_images = np.array(test_images)[ts_indices]\n",
        "      test_labels = np.array(test_labels)[ts_indices]\n",
        "\n",
        "      tr_indices = np.random.permutation(range(len(train_images)))\n",
        "      train_images = np.array(train_images)[tr_indices]\n",
        "      train_labels = np.array(train_labels)[tr_indices]\n",
        "\n",
        "      all_image_batches[b, ...] = np.vstack((train_images, test_images)).reshape(\n",
        "        self.num_samples_per_class, self.num_classes, -1\n",
        "      )\n",
        "      all_label_batches[b, ...] = np.vstack((train_labels, test_labels)).reshape(\n",
        "        self.num_samples_per_class, self.num_classes, -1)\n",
        "    #############################\n",
        "\n",
        "    return all_image_batches, all_label_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wqaX6u_UtjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MANN(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(MANN, self).__init__()\n",
        "    # The first axis is the sequence itself, \n",
        "    # the second indexes instances in the mini-batch, \n",
        "    # and the third indexes elements of the input\n",
        "    self.N = num_classes\n",
        "    self.layer1 = nn.LSTM(784 + num_classes, 128)\n",
        "    self.layer2 = nn.LSTM(128, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\" x is a tensor concatenated images and labels: [(K + 1)*N, B, 784 + N] \"\"\"\n",
        "    # Hint: Passing zeros, not the ground truth labels for the final N examples.\n",
        "    #       Remember we should mask the true labels of test set. \n",
        "    #       Otherwise, our model will learn the pattern and we don't want this.\n",
        "    x[-self.N:, :, -self.N:] = 0\n",
        "    out, _ = self.layer1(x)\n",
        "    out, _ = self.layer2(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgK2TaSRag5t",
        "colab_type": "text"
      },
      "source": [
        "# Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGQCLaZZXdqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Process some flags.')\n",
        "\n",
        "parser.add_argument('--num_classes', type=int, default=2, help='number of classes used in classification (e.g. 5-way classification).')\n",
        "\n",
        "parser.add_argument('--num_samples', type=int, default=1, help='number of examples used for inner gradient update (K for K-shot learning).')\n",
        "\n",
        "parser.add_argument('--meta_batch_size', type=int, default=4, help='Number of N-way classification tasks per batch')\n",
        "\n",
        "parser.add_argument('--data_root', type=str, default= 'omniglot_resized', help='data folder root')\n",
        "\n",
        "FLAGS = parser.parse_args(args=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l94BhW5xa2ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "net = MANN(num_classes=FLAGS.num_classes)\n",
        "net = net.cuda().train()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74ZfpMnhYLYe",
        "colab_type": "code",
        "outputId": "dba37328-499e-4abf-fa3a-d614db2a296c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data_generator = DataGenerator(\n",
        "  FLAGS.num_classes,\n",
        "  FLAGS.num_samples + 1,\n",
        "  {'data_folder': FLAGS.data_root}\n",
        ")\n",
        "\n",
        "for step in range(100000):\n",
        "  net = net.cuda().train()\n",
        "  images, labels = data_generator.sample_batch('train', FLAGS.meta_batch_size)\n",
        "  B, K_plus_1, N, _ = images.shape\n",
        "\n",
        "  images = images.reshape((B, K_plus_1 * N, -1))\n",
        "  labels = labels.reshape((B, K_plus_1 * N, N))\n",
        "\n",
        "  images = torch.tensor(images, device=cuda)\n",
        "  labels = torch.tensor(labels, device=cuda)\n",
        "\n",
        "  optimizer.zero_grad()   # zero the gradient buffers\n",
        "  input_tensor = torch.tensor(torch.cat([images, labels], dim=2).transpose(0, 1), device=cuda)\n",
        "  logits = net(input_tensor.float()).transpose(0, 1)  \n",
        "  # only compute loss for test data which are the final N points\n",
        "  # logits: [B, (K + 1) * N , N]\n",
        "  # labels: [B, (K + 1) * N , N]\n",
        "  loss = criterion(logits[:, -N:], labels[:, -N:].argmax(axis=1))\n",
        "  loss.backward()\n",
        "  optimizer.step()    \n",
        "\n",
        "  if step % 1000 == 0:\n",
        "    with torch.no_grad():\n",
        "      print(\"*\" * 5 + \"Iter \" + str(step) + \"*\" * 5)\n",
        "      net.eval()\n",
        "      images, labels = data_generator.sample_batch('test', 100)\n",
        "      B, K_plus_1, N, _ = images.shape\n",
        "\n",
        "      images = images.reshape((B, K_plus_1 * N, -1))\n",
        "      labels = labels.reshape((B, K_plus_1 * N, N))\n",
        "\n",
        "      images = torch.tensor(images, device=cuda)\n",
        "      labels = torch.tensor(labels, device=cuda)\n",
        "      input_tensor = torch.tensor(torch.cat([images, labels], dim=2).transpose(0, 1), device=cuda)\n",
        "      \n",
        "      logits = net(input_tensor.float()).transpose(0, 1)\n",
        "      test_loss = criterion(logits[:, -N:], labels[:, -N:].argmax(axis=1))\n",
        "      test_loss_value = test_loss.detach().cpu().numpy()\n",
        "\n",
        "      loss_value = loss.detach().cpu().numpy()\n",
        "\n",
        "      print(\"Train Loss:\", loss_value, \"Test Loss:\", test_loss_value)\n",
        "      logits = logits.reshape(\n",
        "        -1, FLAGS.num_samples + 1,\n",
        "        FLAGS.num_classes, FLAGS.num_classes)\n",
        "      \n",
        "      labels = labels.reshape((B, K_plus_1, N, N))\n",
        "\n",
        "      logits = logits[:, -1, :, :].argmax(2)\n",
        "      labels = labels[:, -1, :, :].argmax(2)\n",
        "      accuracy = (1.0 * (logits == labels)).mean()\n",
        "      print(\"Test Accuracy\", accuracy.detach().cpu().numpy())\n",
        "\n",
        "save_path = \"/content/drive/My Drive/cs330/hw1/{}.pth\".format(\"MANN\")\n",
        "torch.save(net.state_dict(), save_path)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "*****Iter 0*****\n",
            "Train Loss: 0.6931702 Test Loss: 0.6931433\n",
            "Test Accuracy 0.49499997\n",
            "*****Iter 1000*****\n",
            "Train Loss: 0.693148 Test Loss: 0.69314224\n",
            "Test Accuracy 0.5\n",
            "*****Iter 2000*****\n",
            "Train Loss: 0.69314307 Test Loss: 0.6931455\n",
            "Test Accuracy 0.505\n",
            "*****Iter 3000*****\n",
            "Train Loss: 0.6930448 Test Loss: 0.6931305\n",
            "Test Accuracy 0.5\n",
            "*****Iter 4000*****\n",
            "Train Loss: 0.69335043 Test Loss: 0.69312114\n",
            "Test Accuracy 0.5\n",
            "*****Iter 5000*****\n",
            "Train Loss: 0.6930114 Test Loss: 0.69327897\n",
            "Test Accuracy 0.5\n",
            "*****Iter 6000*****\n",
            "Train Loss: 0.6932169 Test Loss: 0.6931318\n",
            "Test Accuracy 0.5\n",
            "*****Iter 7000*****\n",
            "Train Loss: 0.69316673 Test Loss: 0.6931458\n",
            "Test Accuracy 0.5\n",
            "*****Iter 8000*****\n",
            "Train Loss: 0.6931713 Test Loss: 0.693145\n",
            "Test Accuracy 0.5\n",
            "*****Iter 9000*****\n",
            "Train Loss: 0.69314456 Test Loss: 0.6931498\n",
            "Test Accuracy 0.5\n",
            "*****Iter 10000*****\n",
            "Train Loss: 0.6931485 Test Loss: 0.6931461\n",
            "Test Accuracy 0.5\n",
            "*****Iter 11000*****\n",
            "Train Loss: 0.6931375 Test Loss: 0.69314694\n",
            "Test Accuracy 0.5\n",
            "*****Iter 12000*****\n",
            "Train Loss: 0.69315076 Test Loss: 0.6931484\n",
            "Test Accuracy 0.5\n",
            "*****Iter 13000*****\n",
            "Train Loss: 0.6931478 Test Loss: 0.6931469\n",
            "Test Accuracy 0.5\n",
            "*****Iter 14000*****\n",
            "Train Loss: 0.69314706 Test Loss: 0.6931468\n",
            "Test Accuracy 0.5\n",
            "*****Iter 15000*****\n",
            "Train Loss: 0.6931466 Test Loss: 0.6931473\n",
            "Test Accuracy 0.5\n",
            "*****Iter 16000*****\n",
            "Train Loss: 0.6931481 Test Loss: 0.69314706\n",
            "Test Accuracy 0.5\n",
            "*****Iter 17000*****\n",
            "Train Loss: 0.6931469 Test Loss: 0.6931472\n",
            "Test Accuracy 0.5\n",
            "*****Iter 18000*****\n",
            "Train Loss: 0.6931466 Test Loss: 0.6931471\n",
            "Test Accuracy 0.5\n",
            "*****Iter 19000*****\n",
            "Train Loss: 0.69314706 Test Loss: 0.69314677\n",
            "Test Accuracy 0.5\n",
            "*****Iter 20000*****\n",
            "Train Loss: 0.69314706 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 21000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 22000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 23000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 24000*****\n",
            "Train Loss: 0.6931475 Test Loss: 0.69314677\n",
            "Test Accuracy 0.5\n",
            "*****Iter 25000*****\n",
            "Train Loss: 0.69314694 Test Loss: 0.69314677\n",
            "Test Accuracy 0.5\n",
            "*****Iter 26000*****\n",
            "Train Loss: 0.69314706 Test Loss: 0.69314665\n",
            "Test Accuracy 0.5\n",
            "*****Iter 27000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 28000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 29000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 30000*****\n",
            "Train Loss: 0.69314706 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 31000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.69314665\n",
            "Test Accuracy 0.5\n",
            "*****Iter 32000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 33000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 34000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 35000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 36000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 37000*****\n",
            "Train Loss: 0.6931471 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 38000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 39000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 40000*****\n",
            "Train Loss: 0.69314694 Test Loss: 0.69314677\n",
            "Test Accuracy 0.5\n",
            "*****Iter 41000*****\n",
            "Train Loss: 0.69314724 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 42000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 43000*****\n",
            "Train Loss: 0.6931471 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 44000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 45000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 46000*****\n",
            "Train Loss: 0.69314706 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 47000*****\n",
            "Train Loss: 0.69314337 Test Loss: 0.69314635\n",
            "Test Accuracy 0.5\n",
            "*****Iter 48000*****\n",
            "Train Loss: 0.6931428 Test Loss: 0.69314677\n",
            "Test Accuracy 0.5\n",
            "*****Iter 49000*****\n",
            "Train Loss: 0.69314593 Test Loss: 0.6931473\n",
            "Test Accuracy 0.5\n",
            "*****Iter 50000*****\n",
            "Train Loss: 0.69314885 Test Loss: 0.6931469\n",
            "Test Accuracy 0.5\n",
            "*****Iter 51000*****\n",
            "Train Loss: 0.6931475 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 52000*****\n",
            "Train Loss: 0.6931473 Test Loss: 0.69314665\n",
            "Test Accuracy 0.5\n",
            "*****Iter 53000*****\n",
            "Train Loss: 0.6931475 Test Loss: 0.6931468\n",
            "Test Accuracy 0.5\n",
            "*****Iter 54000*****\n",
            "Train Loss: 0.6931473 Test Loss: 0.6931468\n",
            "Test Accuracy 0.5\n",
            "*****Iter 55000*****\n",
            "Train Loss: 0.69314706 Test Loss: 0.69314665\n",
            "Test Accuracy 0.5\n",
            "*****Iter 56000*****\n",
            "Train Loss: 0.69314724 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 57000*****\n",
            "Train Loss: 0.69314724 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 58000*****\n",
            "Train Loss: 0.69314706 Test Loss: 0.6931466\n",
            "Test Accuracy 0.5\n",
            "*****Iter 59000*****\n",
            "Train Loss: 0.6931471 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 60000*****\n",
            "Train Loss: 0.6931467 Test Loss: 0.6931469\n",
            "Test Accuracy 0.5\n",
            "*****Iter 61000*****\n",
            "Train Loss: 0.6931477 Test Loss: 0.6931462\n",
            "Test Accuracy 0.5\n",
            "*****Iter 62000*****\n",
            "Train Loss: 0.693146 Test Loss: 0.69314706\n",
            "Test Accuracy 0.5\n",
            "*****Iter 63000*****\n",
            "Train Loss: 0.69314754 Test Loss: 0.6931471\n",
            "Test Accuracy 0.5\n",
            "*****Iter 64000*****\n",
            "Train Loss: 0.6931468 Test Loss: 0.6931469\n",
            "Test Accuracy 0.5\n",
            "*****Iter 65000*****\n",
            "Train Loss: 0.6931471 Test Loss: 0.69314665\n",
            "Test Accuracy 0.5\n",
            "*****Iter 66000*****\n",
            "Train Loss: 0.6931472 Test Loss: 0.69314677\n",
            "Test Accuracy 0.5\n",
            "*****Iter 67000*****\n",
            "Train Loss: 0.69314706 Test Loss: 0.6931465\n",
            "Test Accuracy 0.5\n",
            "*****Iter 68000*****\n",
            "Train Loss: 0.6931464 Test Loss: 0.69314706\n",
            "Test Accuracy 0.5\n",
            "*****Iter 69000*****\n",
            "Train Loss: 0.6929156 Test Loss: 0.69317454\n",
            "Test Accuracy 0.5\n",
            "*****Iter 70000*****\n",
            "Train Loss: 0.69323575 Test Loss: 0.6926267\n",
            "Test Accuracy 0.5\n",
            "*****Iter 71000*****\n",
            "Train Loss: 0.6889542 Test Loss: 0.6943236\n",
            "Test Accuracy 0.5\n",
            "*****Iter 72000*****\n",
            "Train Loss: 0.7168158 Test Loss: 0.64792854\n",
            "Test Accuracy 0.60499996\n",
            "*****Iter 73000*****\n",
            "Train Loss: 0.44329512 Test Loss: 0.6008405\n",
            "Test Accuracy 0.65\n",
            "*****Iter 74000*****\n",
            "Train Loss: 0.39264187 Test Loss: 0.5671444\n",
            "Test Accuracy 0.685\n",
            "*****Iter 75000*****\n",
            "Train Loss: 0.47074643 Test Loss: 0.5595159\n",
            "Test Accuracy 0.7\n",
            "*****Iter 76000*****\n",
            "Train Loss: 0.42847654 Test Loss: 0.5359541\n",
            "Test Accuracy 0.745\n",
            "*****Iter 77000*****\n",
            "Train Loss: 0.7395449 Test Loss: 0.5265812\n",
            "Test Accuracy 0.75\n",
            "*****Iter 78000*****\n",
            "Train Loss: 0.5022837 Test Loss: 0.5015917\n",
            "Test Accuracy 0.76\n",
            "*****Iter 79000*****\n",
            "Train Loss: 0.36713445 Test Loss: 0.49371234\n",
            "Test Accuracy 0.78999996\n",
            "*****Iter 80000*****\n",
            "Train Loss: 0.5401221 Test Loss: 0.48627263\n",
            "Test Accuracy 0.81\n",
            "*****Iter 81000*****\n",
            "Train Loss: 0.42975122 Test Loss: 0.49371868\n",
            "Test Accuracy 0.78999996\n",
            "*****Iter 82000*****\n",
            "Train Loss: 0.38399798 Test Loss: 0.48973396\n",
            "Test Accuracy 0.815\n",
            "*****Iter 83000*****\n",
            "Train Loss: 0.4673418 Test Loss: 0.5390017\n",
            "Test Accuracy 0.73499995\n",
            "*****Iter 84000*****\n",
            "Train Loss: 0.43958333 Test Loss: 0.47535747\n",
            "Test Accuracy 0.805\n",
            "*****Iter 85000*****\n",
            "Train Loss: 0.31527916 Test Loss: 0.4438492\n",
            "Test Accuracy 0.84\n",
            "*****Iter 86000*****\n",
            "Train Loss: 0.39679196 Test Loss: 0.49592212\n",
            "Test Accuracy 0.755\n",
            "*****Iter 87000*****\n",
            "Train Loss: 0.6246042 Test Loss: 0.42329296\n",
            "Test Accuracy 0.885\n",
            "*****Iter 88000*****\n",
            "Train Loss: 0.39312667 Test Loss: 0.45856607\n",
            "Test Accuracy 0.84\n",
            "*****Iter 89000*****\n",
            "Train Loss: 0.4152975 Test Loss: 0.4440754\n",
            "Test Accuracy 0.84499997\n",
            "*****Iter 90000*****\n",
            "Train Loss: 0.6146741 Test Loss: 0.46455616\n",
            "Test Accuracy 0.805\n",
            "*****Iter 91000*****\n",
            "Train Loss: 0.40550506 Test Loss: 0.48173577\n",
            "Test Accuracy 0.815\n",
            "*****Iter 92000*****\n",
            "Train Loss: 0.5514369 Test Loss: 0.46881938\n",
            "Test Accuracy 0.81\n",
            "*****Iter 93000*****\n",
            "Train Loss: 0.6063496 Test Loss: 0.4911638\n",
            "Test Accuracy 0.79499996\n",
            "*****Iter 94000*****\n",
            "Train Loss: 0.44849256 Test Loss: 0.46873224\n",
            "Test Accuracy 0.825\n",
            "*****Iter 95000*****\n",
            "Train Loss: 0.5713399 Test Loss: 0.43743804\n",
            "Test Accuracy 0.84999996\n",
            "*****Iter 96000*****\n",
            "Train Loss: 0.38491443 Test Loss: 0.45163083\n",
            "Test Accuracy 0.81\n",
            "*****Iter 97000*****\n",
            "Train Loss: 0.4273776 Test Loss: 0.47642604\n",
            "Test Accuracy 0.81\n",
            "*****Iter 98000*****\n",
            "Train Loss: 0.28069434 Test Loss: 0.46204787\n",
            "Test Accuracy 0.81\n",
            "*****Iter 99000*****\n",
            "Train Loss: 0.57082254 Test Loss: 0.41151398\n",
            "Test Accuracy 0.865\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}